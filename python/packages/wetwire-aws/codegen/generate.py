"""
Stage 3: Generate Python dataclasses from intermediate format.

Reads the parsed intermediate schema and generates Python source files
for each AWS service, including enum constants from botocore.
"""

import argparse
import json
import os
import subprocess
import sys
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import UTC, datetime
from pathlib import Path

from codegen.config import GENERATOR_VERSION, RESOURCES_DIR, SPECS_DIR
from wetwire.codegen import (
    IntermediateSchema,
    NestedTypeDef,
    PropertyDef,
    ResourceDef,
)
from wetwire.codegen.generator import python_type_for_property, format_file

# Use all available CPU cores for parallel generation
NUM_WORKERS = os.cpu_count() or 4


# Mapping from CloudFormation service names to botocore service names
CF_TO_BOTOCORE_SERVICE = {
    "lambda_": "lambda",
    "apigateway": "apigateway",
    "apigatewayv2": "apigatewayv2",
    "applicationautoscaling": "application-autoscaling",
    "autoscaling": "autoscaling",
    "certificatemanager": "acm",
    "cloudfront": "cloudfront",
    "cloudwatch": "cloudwatch",
    "codebuild": "codebuild",
    "codepipeline": "codepipeline",
    "cognito": "cognito-idp",
    "dynamodb": "dynamodb",
    "ec2": "ec2",
    "ecr": "ecr",
    "ecs": "ecs",
    "efs": "efs",
    "elasticache": "elasticache",
    "elasticbeanstalk": "elasticbeanstalk",
    "elasticloadbalancing": "elb",
    "elasticloadbalancingv2": "elbv2",
    "events": "events",
    "iam": "iam",
    "kinesis": "kinesis",
    "kms": "kms",
    "logs": "logs",
    "rds": "rds",
    "redshift": "redshift",
    "route53": "route53",
    "s3": "s3",
    "secretsmanager": "secretsmanager",
    "ses": "ses",
    "sns": "sns",
    "sqs": "sqs",
    "ssm": "ssm",
    "stepfunctions": "stepfunctions",
    "wafv2": "wafv2",
}


def generate_file_header(
    service: str,
    cf_spec_version: str,
    generator_version: str,
    import_tag: bool = True,
) -> str:
    """Generate the file header with version info.

    Args:
        service: Service name
        cf_spec_version: CloudFormation spec version
        generator_version: Generator version
        import_tag: Whether to import Tag from base (False if service has a Tag resource)
    """
    timestamp = datetime.now(UTC).strftime("%Y-%m-%dT%H:%M:%SZ")

    # Build import line - only import Tag if no Tag resource exists in this service
    if import_tag:
        base_imports = "CloudFormationResource, PropertyType, Tag"
    else:
        base_imports = "CloudFormationResource, PropertyType"

    return f'''"""
AWS {service.upper()} CloudFormation resources.

Generated:
  Source: CloudFormation Spec {cf_spec_version}
  Generator: {generator_version}
  Date: {timestamp}

DO NOT EDIT - This file is generated by wetwire-aws codegen.
To regenerate: python -m wetwire_aws.codegen.generate
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, ClassVar

from wetwire_aws.base import {base_imports}

'''


def get_property_type_signature(nested: NestedTypeDef) -> str:
    """Get a signature string for a PropertyType to detect duplicates."""
    if not nested.properties:
        return ""
    props = sorted((p.name, python_type_for_property(p)) for p in nested.properties)
    return str(props)


def get_unique_nested_name(
    nested: NestedTypeDef,
    duplicates: set[str],
    resource_collisions: set[str],
    enum_names: set[str] | None = None,
) -> str:
    """
    Get a unique class name for a nested type.

    Handles two cases:
    1. Duplicate nested type names (same name, different structures)
       -> Prefix with parent resource name: "LoggingConfiguration_FieldToMatch"
    2. Nested type name collides with a Resource class name or reserved name
       -> Add "Type" suffix: "SamplingRuleType"
       -> If "Type" suffix also collides with enum, use "PropertyType" suffix

    Args:
        nested: The nested type definition
        duplicates: Set of nested type names that appear multiple times
        resource_collisions: Set of nested type names that match resource names
        enum_names: Set of enum names to avoid collisions with

    Returns:
        Unique class name for the nested type
    """
    name = nested.name
    enum_names = enum_names or set()

    # Handle duplicates by prefixing with parent resource name
    if name in duplicates:
        # original_name = "AWS::WAFv2::LoggingConfiguration.FieldToMatch"
        # Extract parent: "LoggingConfiguration"
        if "." in nested.original_name:
            parts = nested.original_name.split(".")
            if len(parts) >= 2:
                # Get the resource name part (before the last dot)
                resource_part = parts[-2]  # e.g., "AWS::WAFv2::LoggingConfiguration"
                parent = resource_part.split("::")[-1]  # e.g., "LoggingConfiguration"
                name = f"{parent}_{name}"  # e.g., "LoggingConfiguration_FieldToMatch"

    # Handle resource collisions by adding "Type" suffix
    if name in resource_collisions or nested.name in resource_collisions:
        new_name = f"{name}Type"
        # If the "Type" suffix collides with an enum, use "PropertyType" instead
        if new_name in enum_names:
            name = f"{name}PropertyType"
        else:
            name = new_name

    return name


def generate_property_type(
    nested: NestedTypeDef,
    class_name: str | None = None,
    name_mapping: dict[str, str] | None = None,
    nested_type_names: set[str] | None = None,
) -> str:
    """Generate a PropertyType class for a nested structure.

    Args:
        nested: The nested type definition
        class_name: Optional override for the class name
        name_mapping: Optional mapping from short nested type names to unique names
        nested_type_names: Set of all nested type names in this service
    """
    lines = []
    name_mapping = name_mapping or {}
    nested_type_names = nested_type_names or set()

    # Use provided class_name or default to nested.name
    name = class_name if class_name else nested.name

    # Class definition
    lines.append("@dataclass")
    lines.append(f"class {name}(PropertyType):")

    # Skip URL-only docstrings to reduce file size
    # Documentation URLs can be derived from class name if needed

    if not nested.properties:
        lines.append("    pass")
        return "\n".join(lines)

    # Generate properties - all optional to avoid dataclass inheritance issues
    for prop in nested.properties:
        python_type = python_type_for_property(prop)

        # Check if type is a nested type from this service - apply name mapping
        if prop.nested_type and prop.nested_type in nested_type_names:
            type_name = name_mapping.get(prop.nested_type, prop.nested_type)
            if prop.is_list:
                python_type = f"list[{type_name}]"
            elif prop.is_map:
                python_type = f"dict[str, {type_name}]"
            else:
                python_type = type_name

        # All fields are optional to avoid dataclass inheritance issues
        if python_type.startswith("list"):
            lines.append(
                f"    {prop.name}: {python_type} = field(default_factory=list)"
            )
        elif python_type.startswith("dict"):
            lines.append(
                f"    {prop.name}: {python_type} = field(default_factory=dict)"
            )
        else:
            lines.append(f"    {prop.name}: {python_type} | None = None")

    return "\n".join(lines)


def generate_enum_class(enum_name: str, enum_data: dict) -> str:
    """Generate a string constants class for an enum."""
    lines = []
    lines.append(f"class {enum_name}:")

    values = enum_data.get("values", [])
    if not values:
        lines.append("    pass")
        return "\n".join(lines)

    for val in values:
        name = val["name"]
        value = val["value"]
        # Escape quotes in value
        escaped_value = value.replace('"', '\\"')
        lines.append(f'    {name} = "{escaped_value}"')

    return "\n".join(lines)


def load_enums_for_service(service: str) -> dict[str, dict]:
    """Load enum definitions for a service from enums.json."""
    enums_path = SPECS_DIR / "enums.json"
    if not enums_path.exists():
        return {}

    try:
        all_enums = json.loads(enums_path.read_text())
        services = all_enums.get("services", {})

        # Try direct match first
        if service in services:
            return services[service]

        # Try botocore service name mapping
        botocore_name = CF_TO_BOTOCORE_SERVICE.get(service, service)
        if botocore_name in services:
            return services[botocore_name]

        return {}
    except Exception:
        return {}


def get_all_enums_for_service(
    service: str, all_enums: dict[str, dict]
) -> list[tuple[str, dict]]:
    """Get all enums for a service."""
    return [(name, data) for name, data in sorted(all_enums.items())]


def generate_resource_class(
    resource: ResourceDef,
    nested_types: set[str],
    name_mapping: dict[str, str] | None = None,
) -> str:
    """Generate a CloudFormationResource class.

    Args:
        resource: The resource definition
        nested_types: Set of original nested type names in this service
        name_mapping: Optional mapping from original nested type names to unique names
    """
    # Field names reserved by CloudFormationResource base class (different types)
    BASE_CLASS_FIELDS = {"update_policy", "creation_policy", "deletion_policy", "depends_on"}

    lines = []
    name_mapping = name_mapping or {}

    # Class definition
    lines.append("@dataclass")
    lines.append(f"class {resource.name}(CloudFormationResource):")

    # Docstring
    doc_lines = [f'"""{resource.name} resource.']
    if resource.documentation:
        doc_lines.append("")
        doc_lines.append(resource.documentation)
    doc_lines.append("")
    doc_lines.append(f"CloudFormation type: {resource.full_type}")
    doc_lines.append('"""')
    lines.append("    " + "\n    ".join(doc_lines))
    lines.append("")

    # Class variables
    lines.append(f'    _resource_type: ClassVar[str] = "{resource.full_type}"')
    lines.append("")

    if not resource.properties:
        lines.append("    pass")
        return "\n".join(lines)

    # Generate properties - all optional to avoid dataclass inheritance issues
    for prop in resource.properties:
        python_type = python_type_for_property(prop)
        field_name = prop.name

        # Rename fields that conflict with base class fields
        if field_name in BASE_CLASS_FIELDS:
            field_name = f"resource_{field_name}"

        # Check if type is a nested type from this service
        if prop.nested_type and prop.nested_type in nested_types:
            # Use mapped name if available (handles duplicates and collisions)
            type_name = name_mapping.get(prop.nested_type, prop.nested_type)
            if prop.is_list:
                python_type = f"list[{type_name}]"
            elif prop.is_map:
                python_type = f"dict[str, {type_name}]"
            else:
                python_type = type_name

        # All fields are optional to avoid dataclass inheritance issues
        if python_type.startswith("list"):
            lines.append(
                f"    {field_name}: {python_type} = field(default_factory=list)"
            )
        elif python_type.startswith("dict") and not python_type.startswith(
            "dict[str, Any]"
        ):
            lines.append(
                f"    {field_name}: {python_type} = field(default_factory=dict)"
            )
        else:
            lines.append(f"    {field_name}: {python_type} | None = None")

    # Generate attribute accessors if there are attributes
    if resource.attributes:
        lines.append("")
        lines.append("    # GetAtt attributes")
        for attr in resource.attributes:
            attr_const = attr.name.upper().replace("-", "_").replace(".", "_")
            lines.append(f'    {attr_const}: ClassVar[str] = "{attr.name}"')

    return "\n".join(lines)


def generate_service_module(
    service: str,
    resources: list[ResourceDef],
    nested_types: list[NestedTypeDef],
    cf_spec_version: str,
    service_enums: list[tuple[str, dict]] | None = None,
) -> str:
    """Generate a complete service module."""
    lines = []

    # Check if there's a Tag resource in this service (don't import Tag from base if so)
    has_tag_resource = any(r.name == "Tag" for r in resources)

    # Header
    lines.append(generate_file_header(service, cf_spec_version, GENERATOR_VERSION, import_tag=not has_tag_resource))

    # Collect nested type names for this service
    nested_type_names = {n.name for n in nested_types}

    # Detect name collisions
    # 1. Duplicate nested type names (same name, different structures)
    nested_name_counts: dict[str, int] = {}
    for n in nested_types:
        nested_name_counts[n.name] = nested_name_counts.get(n.name, 0) + 1
    duplicates = {name for name, count in nested_name_counts.items() if count > 1}

    # 2. Nested type names that collide with resource class names, imports, or enums
    resource_names = {r.name for r in resources}
    # Also include imported names that should never be shadowed
    reserved_names = {"PropertyType", "Tag", "CloudFormationResource"}
    # Get enum names to check for collisions
    enum_names_set = {e[0] for e in service_enums} if service_enums else set()
    resource_collisions = (nested_type_names & resource_names) | (nested_type_names & reserved_names) | (nested_type_names & enum_names_set)

    # Build name mappings for property type resolution
    # For duplicates, we need resource-specific mappings
    # Map from (resource_name, nested_type_name) -> unique_class_name
    nested_by_original: dict[str, NestedTypeDef] = {n.original_name: n for n in nested_types}

    # For each resource, build a mapping of nested_type short name -> unique class name
    # based on which nested types belong to that resource
    resource_name_mappings: dict[str, dict[str, str]] = {}
    for resource in resources:
        resource_mapping: dict[str, str] = {}
        for nested in nested_types:
            # Check if this nested type belongs to this resource
            # original_name format: "AWS::WAFv2::LoggingConfiguration.FieldToMatch"
            if "." in nested.original_name:
                parts = nested.original_name.split(".")
                parent_resource = parts[-2].split("::")[-1]  # e.g., "LoggingConfiguration"
                if parent_resource == resource.name:
                    unique_name = get_unique_nested_name(nested, duplicates, resource_collisions, enum_names_set)
                    resource_mapping[nested.name] = unique_name
        resource_name_mappings[resource.name] = resource_mapping

    # Also build a general mapping for non-duplicate types (used as fallback)
    general_name_mapping: dict[str, str] = {}
    for nested in nested_types:
        if nested.name not in duplicates:
            unique_name = get_unique_nested_name(nested, duplicates, resource_collisions, enum_names_set)
            general_name_mapping[nested.name] = unique_name

    # Generate enum constants first (single blank line between)
    # Reserved names that enums shouldn't shadow
    reserved_enum_names = {"PropertyType", "Tag", "CloudFormationResource", "Any", "ClassVar"}
    enum_names_set = {e[0] for e in service_enums} if service_enums else set()

    enum_names = []
    if service_enums:
        lines.append("# Constants")
        lines.append("# " + "=" * 60)

        for enum_name, enum_data in sorted(service_enums, key=lambda x: x[0]):
            # Rename enums that conflict with imports, resources, or nested types
            actual_name = enum_name
            if enum_name in reserved_enum_names or enum_name in resource_names or enum_name in nested_type_names:
                actual_name = f"{enum_name}Values"  # e.g., PropertyType -> PropertyTypeValues
            lines.append("")
            lines.append(generate_enum_class(actual_name, enum_data))
            enum_names.append(actual_name)
        lines.append("")

    # Build nested-type-specific name mappings (similar to resource mappings)
    # Each nested type needs a mapping for its sibling types (from same parent)
    nested_name_mappings: dict[str, dict[str, str]] = {}
    for nested in nested_types:
        # Extract parent from original_name: "AWS::WAFv2::RuleGroup.ByteMatchStatement"
        if "." in nested.original_name:
            parts = nested.original_name.split(".")
            parent_resource = parts[-2].split("::")[-1]  # e.g., "RuleGroup"

            # Build mapping for this nested type's parent
            if parent_resource not in nested_name_mappings:
                nested_mapping: dict[str, str] = {}
                for other in nested_types:
                    if "." in other.original_name:
                        other_parts = other.original_name.split(".")
                        other_parent = other_parts[-2].split("::")[-1]
                        if other_parent == parent_resource:
                            unique_name = get_unique_nested_name(other, duplicates, resource_collisions, enum_names_set)
                            nested_mapping[other.name] = unique_name
                nested_name_mappings[parent_resource] = nested_mapping

    # Generate nested types with proper unique naming
    # Track generated class names to create aliases for signature-identical types
    seen_signatures: dict[str, str] = {}  # signature -> first generated class name
    generated_names: set[str] = set()  # Track what we've generated
    aliases: list[tuple[str, str]] = []  # (alias_name, target_name)

    if nested_types:
        lines.append("")
        lines.append("# Property Types")
        lines.append("# " + "=" * 60)

        for nested in sorted(nested_types, key=lambda n: n.original_name):
            unique_name = get_unique_nested_name(nested, duplicates, resource_collisions, enum_names_set)
            sig = get_property_type_signature(nested)

            if unique_name in generated_names:
                # Already generated this class (e.g., same unique name from same parent)
                continue

            if sig and sig in seen_signatures:
                # Same signature as existing class - create alias
                aliases.append((unique_name, seen_signatures[sig]))
            else:
                # Get the name mapping for this nested type's parent
                parent_resource = ""
                if "." in nested.original_name:
                    parts = nested.original_name.split(".")
                    parent_resource = parts[-2].split("::")[-1]

                # Merge general and parent-specific mappings
                combined_mapping = dict(general_name_mapping)
                combined_mapping.update(nested_name_mappings.get(parent_resource, {}))

                lines.append("")
                lines.append(generate_property_type(
                    nested, unique_name, combined_mapping, nested_type_names
                ))
                generated_names.add(unique_name)
                if sig:
                    seen_signatures[sig] = unique_name

        # Generate aliases for signature-identical types
        if aliases:
            lines.append("")
            lines.append("")
            lines.append("# Type aliases for duplicate property structures")
            for alias_name, original_name in sorted(aliases):
                lines.append(f"{alias_name} = {original_name}")
        lines.append("")

    # Generate resources (single blank line between)
    lines.append("")
    lines.append("# Resources")
    lines.append("# " + "=" * 60)

    for resource in sorted(resources, key=lambda r: r.name):
        # Merge resource-specific mapping with general mapping
        # Resource-specific takes precedence for duplicates
        combined_mapping = dict(general_name_mapping)
        combined_mapping.update(resource_name_mappings.get(resource.name, {}))

        lines.append("")
        lines.append(generate_resource_class(resource, nested_type_names, combined_mapping))
    lines.append("")

    # Dynamic __all__ generation
    lines.append("")
    lines.append("# Export all public classes dynamically")
    lines.append("def _get_all() -> list[str]:")
    lines.append("    import sys")
    lines.append("    return [")
    lines.append("        name for name, obj in vars(sys.modules[__name__]).items()")
    lines.append("        if isinstance(obj, type) and not name.startswith('_')")
    lines.append("    ]")
    lines.append("")
    lines.append("__all__ = _get_all()")

    return "\n".join(lines)


def generate_init_file(services: list[str], cf_spec_version: str) -> str:
    """Generate the resources/__init__.py file."""
    timestamp = datetime.now(UTC).strftime("%Y-%m-%dT%H:%M:%SZ")

    lines = [
        '"""',
        "AWS CloudFormation resource types.",
        "",
        "This package contains generated resource classes for all AWS services.",
        "",
        "Generated:",
        f"  Source: CloudFormation Spec {cf_spec_version}",
        f"  Generator: {GENERATOR_VERSION}",
        f"  Date: {timestamp}",
        '"""',
        "",
    ]

    # Import all services
    for service in sorted(services):
        lines.append(f"from wetwire_aws.resources import {service}")

    lines.append("")
    lines.append("__all__ = [")
    for service in sorted(services):
        lines.append(f'    "{service}",')
    lines.append("]")

    return "\n".join(lines)


def generate(
    format_code: bool = True, dry_run: bool = False, include_enums: bool = True
) -> None:
    """
    Run the generate stage.

    Generates Python source files from the intermediate schema.
    """
    print("Stage 3: Generate")
    print("=" * 40)

    # Load intermediate schema
    parsed_path = SPECS_DIR / "parsed.json"
    if not parsed_path.exists():
        raise FileNotFoundError("parsed.json not found. Run parse first.")

    print(f"Loading {parsed_path}...")
    schema = IntermediateSchema.from_dict(json.loads(parsed_path.read_text()))

    # Group resources and nested types by service
    resources_by_service: dict[str, list[ResourceDef]] = defaultdict(list)
    nested_by_service: dict[str, list[NestedTypeDef]] = defaultdict(list)

    for resource in schema.resources:
        resources_by_service[resource.service].append(resource)

    for nested in schema.nested_types:
        nested_by_service[nested.service].append(nested)

    services = sorted(set(resources_by_service.keys()) | set(nested_by_service.keys()))
    print(f"Generating {len(services)} service modules...")

    # Load enums if available
    enums_by_service: dict[str, list[tuple[str, dict]]] = {}
    if include_enums:
        enums_path = SPECS_DIR / "enums.json"
        if enums_path.exists():
            print("Loading enum constants from botocore...")
            try:
                all_enums = json.loads(enums_path.read_text())
                # Validate enums file has expected structure
                if "services" not in all_enums:
                    print("  Warning: enums.json missing 'services' key")

                for service in services:
                    # Load all enums for the service
                    service_enums = load_enums_for_service(service)
                    if service_enums:
                        # Get all enums for this service
                        all_service_enums = get_all_enums_for_service(
                            service, service_enums
                        )
                        if all_service_enums:
                            enums_by_service[service] = all_service_enums
            except Exception as e:
                print(f"  Warning: Could not load enums: {e}")

    if dry_run:
        print("\nDry run - would generate:")
        for service in services:
            resource_count = len(resources_by_service.get(service, []))
            nested_count = len(nested_by_service.get(service, []))
            enum_count = len(enums_by_service.get(service, []))
            parts = [f"{resource_count} resources", f"{nested_count} property types"]
            if enum_count:
                parts.append(f"{enum_count} enums")
            print(f"  {service}: {', '.join(parts)}")
        return

    # Ensure resources directory exists
    RESOURCES_DIR.mkdir(parents=True, exist_ok=True)

    def generate_single_service(service: str) -> tuple[Path, int, int, int]:
        """Generate a single service module."""
        service_resources = resources_by_service.get(service, [])
        service_nested = nested_by_service.get(service, [])
        service_enums = enums_by_service.get(service)

        # Generate module content
        content = generate_service_module(
            service,
            service_resources,
            service_nested,
            schema.source_version,
            service_enums=service_enums,
        )

        # Write file
        service_dir = RESOURCES_DIR / service
        service_dir.mkdir(exist_ok=True)

        init_path = service_dir / "__init__.py"
        init_path.write_text(content)

        resource_count = len(service_resources)
        nested_count = len(service_nested)
        enum_count = len(service_enums) if service_enums else 0

        return init_path, resource_count, nested_count, enum_count

    # Generate service modules in parallel
    generated_files: list[Path] = []
    total_enums = 0

    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
        futures = {
            executor.submit(generate_single_service, svc): svc for svc in services
        }

        for future in as_completed(futures):
            service = futures[future]
            try:
                init_path, resource_count, nested_count, enum_count = future.result()
                generated_files.append(init_path)
                total_enums += enum_count

                parts = [
                    f"{resource_count} resources",
                    f"{nested_count} property types",
                ]
                if enum_count:
                    parts.append(f"{enum_count} enums")
                print(f"  {service}: {', '.join(parts)}")
            except Exception as e:
                print(f"  {service}: ERROR - {e}")

    # Generate main __init__.py (not parallelized - single file)
    init_content = generate_init_file(services, schema.source_version)
    init_path = RESOURCES_DIR / "__init__.py"
    init_path.write_text(init_content)
    generated_files.append(init_path)

    print(f"\nGenerated {len(generated_files)} files")
    if total_enums:
        print(f"  Including {total_enums} enum constant classes")

    # Format with black in parallel
    if format_code:
        print("\nFormatting with black...")
        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
            results = list(executor.map(format_file, generated_files))
        formatted = sum(results)
        print(f"  Formatted {formatted}/{len(generated_files)} files")

    print(f"\nOutput written to {RESOURCES_DIR}")


def main():
    parser = argparse.ArgumentParser(
        description="Generate Python dataclasses from intermediate schema"
    )
    parser.add_argument(
        "--no-format", action="store_true", help="Skip black formatting"
    )
    parser.add_argument(
        "--no-enums", action="store_true", help="Skip enum constant generation"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be generated without writing",
    )
    args = parser.parse_args()

    try:
        generate(
            format_code=not args.no_format,
            dry_run=args.dry_run,
            include_enums=not args.no_enums,
        )
        print("\nGenerate completed successfully!")
    except Exception as e:
        print(f"\nGenerate failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
